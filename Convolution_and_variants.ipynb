{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and Variants\n",
    "In last post, we discussed some famous Convolutional Neural Networks. But that was Part 1 of the ***Famous Convolutional Neural Network Architectures*** series. Before going to Part 2 we need to go over some interesting variants of Convolution Operations! So let's start.\n",
    "\n",
    "## Index\n",
    "<ul>\n",
    "    <li>Simple Convolution</li>\n",
    "    <li>Network in Network (a.k.a. 1x1 Convolution)</li>\n",
    "    <li>Depthwise Separable Convolutions</li>\n",
    "    <li>Transposed Convolutions</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convolution\n",
    "In a [previous post], we studied about basics of convolutions in detail. Let's take a gist of it here! We take a volume of specific numbers (called kernel) having size smaller than (sometimes equal to) the input volume, with varying volume depth. This kernel when applied to the input volume performs following operations:\n",
    "\n",
    "1. Start from the top left corner of the input volume.\n",
    "2. Take slice of input volume from current position such that the slice is equal in field view as the kernel.\n",
    "3. Apply element-wise multiplication between slice and kernel.\n",
    "4. Add result over the axis that represents number of channels (in case of image, the axis representing the RGB channels)\n",
    "5. (if possible) Stride `s` steps ahead from current position\n",
    "6. (else) Stride `s` steps down from current position and begin from leftmost corner.\n",
    "7. Repeat steps 2 to 7 until you cannot go further.\n",
    "\n",
    "The animation below describes how one of the many kernels present in the convolution layer works. We have a kernel with 3 channels and 3x3 size convolving over the input volume\n",
    "\n",
    "![](images/convolve.gif)\n",
    "\n",
    "\n",
    "Try Basic Conv yourself using [this excel sheet](excels/basicCNNs.xlsx) I curated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 Convolutions (a.k.a Network in Network)\n",
    "Consider having a Multi-layer Perceptron that is embedded inside your convolutional Network without the need to flatten the image! But how does this helps us? Let's consider benefits of having a 1x1 Conv\n",
    "\n",
    "1. Reducing or increasing the number of channels in the input volume. This is helpful when we need to do branching or we need to create a depth based bottleneck. Bottlenecks in models usually force models to find meaningful representations.\n",
    "2. Can be used to reduce the number of weights in a network block.\n",
    "  * Consider having an input volume of 128x128 with 256 channels. If we apply 512 kernels of 3x3 size to this input volume, then the number of kernel weights required  equal 3x3x256x512 = 1,179,648 (not considering bias).\n",
    "  * Now let's add a 1x1 convoltion with 64 kernels between the input volume and the convolution layer with 512 kernels of 3x3. Weights in 1x1 conv layer becomes 1x1x64x256 = 16,384 and weights in the 3x3 conv layer becomes 3x3x64x512 = 294,912. Hence, the total number of weights in this new setting becomes 311,296. We just dropped the number of weights by almost a factor of 4.\n",
    "\n",
    "The below animation describes working of 1x1 convs aptly.\n",
    "\n",
    "![](images/1x1conv.gif)\n",
    "\n",
    "\n",
    "Try 1x1 Conv yourself using [this excel sheet](excels/1x1convs.xlsx) I curated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depthwise Separable Convolutions\n",
    "Depthwise Separable Convolutions are two step convolutions that came into existence as a solution to two main issues with simple convolutions:\n",
    "* Reduce the complexity of the convolutional layer\n",
    "* Reduce the number of parameters required\n",
    "\n",
    "The two steps involved in separable convolutions are:\n",
    "1. Depthwise Convolution\n",
    "2. Pointwise Convolution\n",
    "\n",
    "### Depthwise Convolution\n",
    "As the name suggests, we perform kernel on depth of the input volume (on the input channels). The steps followed in this convolution are:\n",
    "1. Take number of kernels equal to the number of input channels. Each kernel having depth 1. Example, if we have a kernel of size 3x3 and an input of size 6x6 with 16 channels then there will be 16 3x3 kernels.\n",
    "2. Every channel thus have 1 kernel associated with it. This kernel is convolved over the associated channel separately. Resulting in 16 feature maps\n",
    "3. Stack all these feature maps to get the output volume with 4x4 output size and 16 channels.\n",
    "\n",
    "![](images/depthwise_conv.gif)\n",
    "\n",
    "### Pointwise Convolution\n",
    "Again, as name suggests this type of convolution is applied to ever single point in the convolution separately (remember 1x1 convs?). So how does this work?\n",
    "1. Take a 1x1 conv with number of filters equal to number of channels you want as output.\n",
    "2. Perform basic convolution applied in 1x1 conv to the output of the Depthwise convolution.\n",
    "\n",
    "Well, too fond of animation? See the 1x1 conv animation again \\\\_( ^ _ ^ )_/\n",
    "\n",
    "\n",
    "Try Depthwise Separable Conv yourself using [this excel sheet](excels/depthwise_separable_convs.xlsx) I curated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposed Convolutions\n",
    "In coming posts we will be in need to upsample the tensors that were downsampled by convolutional layers. We might need to define an image segmentation model or generative model, these models are well-known and use transposed convolutions extensively.\n",
    "\n",
    "> Side note: In some materials you will see name deconvolution at places where we need transposed convolution. I might use it in future posts as well. In mathematical terms tranpose convolution and deconvolution are two different operations. But we will address tranposed convolution (operation) as deconvolution at times.\n",
    "\n",
    "Moving on, let's understand how tranposed convolutions work:\n",
    "1. Take the input volume and add 0s at each alternate position. This gives us a new volume.\n",
    "2. Apply basic convolution on this new volume with the kernels.\n",
    "\n",
    "The below animation describes it pretty well.\n",
    "\n",
    "![](images/transposed_conv.gif)\n",
    "\n",
    "\n",
    "Try Transposed Conv yourself using [this excel sheet](excels/transposed_convs.xlsx) I curated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a lot of variants of Convolutions, but these are some basic once, which generally are used much more often than others!\n",
    "\n",
    "Let's clarify each other's doubts in the comments below :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
